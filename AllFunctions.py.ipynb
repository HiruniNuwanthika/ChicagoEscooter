{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5bb0798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import datetime\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ce730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateSetPerHour(day):\n",
    "    for hour in range(0,24):\n",
    "#first part of the data\n",
    "        data_frames1 = []\n",
    "        files1=[]\n",
    "        for file_id in range(1,16):\n",
    "            try:\n",
    "                with open(r'C:\\Melbourne_Escooter\\RealDataset\\August\\8_{}t{}_{}.json'.format(day,hour,file_id),'r') as f:\n",
    "                    data = json.loads(f.read())\n",
    "                df_normalized=pd.json_normalize(data, ['data', 'bikes'])\n",
    "                df_normalized_escooter=df_normalized[df_normalized['vehicle_type']=='scooter']\n",
    "                df_normalized_escooter.drop(['is_reserved', 'is_disabled','vehicle_type_id','pricing_plan_id','last_reported','vehicle_type'], axis=1, inplace=True)\n",
    "                df=pd.DataFrame(df_normalized_escooter)\n",
    "                data_frames1.append(df)\n",
    "                files1.append(file_id)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        if(len(data_frames1)!=0):\n",
    "            df_merged1 = reduce(lambda  left,right: pd.merge(left,right,on=['bike_id'],suffixes=('_a', '_b'),\n",
    "                                                   how='outer'), data_frames1)\n",
    "            #create dynamic column names\n",
    "            columns1=['bike_id']\n",
    "            for i in files1:\n",
    "                columns1.append('lat_%s' %i)\n",
    "                columns1.append('lon_%s' %i)\n",
    "                columns1.append('range_%s' %i)\n",
    "            #print(columns)\n",
    "            #add created colunm names\n",
    "            df_merged1.columns = columns1\n",
    "            #save file\n",
    "            df_merged1.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part1.csv'.format(day,hour), index=False)\n",
    "#second part of the data\n",
    "        data_frames2= []\n",
    "        files2=[]\n",
    "        for file_id in range(16,31):\n",
    "            try:\n",
    "                with open(r'C:\\Melbourne_Escooter\\RealDataset\\August\\8_{}t{}_{}.json'.format(day,hour,file_id),'r') as f:\n",
    "                    data = json.loads(f.read())\n",
    "                df_normalized=pd.json_normalize(data, ['data', 'bikes'])\n",
    "                df_normalized_escooter=df_normalized[df_normalized['vehicle_type']=='scooter']\n",
    "                df_normalized_escooter.drop(['is_reserved', 'is_disabled','vehicle_type_id','pricing_plan_id','last_reported','vehicle_type'], axis=1, inplace=True)\n",
    "                df=pd.DataFrame(df_normalized_escooter)\n",
    "                data_frames2.append(df)\n",
    "                files2.append(file_id)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        if(len(data_frames2)!=0):\n",
    "            df_merged2 = reduce(lambda  left,right: pd.merge(left,right,on=['bike_id'],suffixes=('_a', '_b'),\n",
    "                                                   how='outer'), data_frames2)\n",
    "            #create dynamic column names\n",
    "            columns2=['bike_id']\n",
    "            for i in files2:\n",
    "                columns2.append('lat_%s' %i)\n",
    "                columns2.append('lon_%s' %i)\n",
    "                columns2.append('range_%s' %i)\n",
    "            #add created colunm names\n",
    "            df_merged2.columns = columns2\n",
    "            #save file\n",
    "            df_merged2.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part2.csv'.format(day,hour), index=False)\n",
    "#Third part of the dataset\n",
    "        data_frames3= []\n",
    "        files3=[]\n",
    "        for file_id in range(31,46):\n",
    "            try:\n",
    "                with open(r'C:\\Melbourne_Escooter\\RealDataset\\August\\8_{}t{}_{}.json'.format(day,hour,file_id),'r') as f:\n",
    "                    data = json.loads(f.read())\n",
    "                df_normalized=pd.json_normalize(data, ['data', 'bikes'])\n",
    "                df_normalized_escooter=df_normalized[df_normalized['vehicle_type']=='scooter']\n",
    "                df_normalized_escooter.drop(['is_reserved', 'is_disabled','vehicle_type_id','pricing_plan_id','last_reported','vehicle_type'], axis=1, inplace=True)\n",
    "                df=pd.DataFrame(df_normalized_escooter)\n",
    "                data_frames3.append(df)\n",
    "                files3.append(file_id)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        if(len(data_frames3)!=0):\n",
    "            df_merged3 = reduce(lambda  left,right: pd.merge(left,right,on=['bike_id'],suffixes=('_a', '_b'),\n",
    "                                                   how='outer'), data_frames3)\n",
    "            #create dynamic column names\n",
    "            columns3=['bike_id']\n",
    "            for i in files3:\n",
    "                columns3.append('lat_%s' %i)\n",
    "                columns3.append('lon_%s' %i)\n",
    "                columns3.append('range_%s' %i)\n",
    "            #add created colunm names\n",
    "            df_merged3.columns = columns3\n",
    "            #save file\n",
    "            df_merged3.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part3.csv'.format(day,hour), index=False)\n",
    "#fourth part of the dataset\n",
    "        data_frames4= []\n",
    "        files4=[]\n",
    "        for file_id in range(46,60):\n",
    "            try:\n",
    "                with open(r'C:\\Melbourne_Escooter\\RealDataset\\August\\8_{}t{}_{}.json'.format(day,hour,file_id),'r') as f:\n",
    "                    data = json.loads(f.read())\n",
    "                df_normalized=pd.json_normalize(data, ['data', 'bikes'])\n",
    "                df_normalized_escooter=df_normalized[df_normalized['vehicle_type']=='scooter']\n",
    "                df_normalized_escooter.drop(['is_reserved', 'is_disabled','vehicle_type_id','pricing_plan_id','last_reported','vehicle_type'], axis=1, inplace=True)\n",
    "                df=pd.DataFrame(df_normalized_escooter)\n",
    "                data_frames4.append(df)\n",
    "                files4.append(file_id)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        if(len(data_frames4)!=0):\n",
    "            df_merged4 = reduce(lambda  left,right: pd.merge(left,right,on=['bike_id'],suffixes=('_a', '_b'),\n",
    "                                                   how='outer'), data_frames4)\n",
    "            #create dynamic column names\n",
    "            columns4=['bike_id']\n",
    "            for i in files4:\n",
    "                columns4.append('lat_%s' %i)\n",
    "                columns4.append('lon_%s' %i)\n",
    "                columns4.append('range_%s' %i)\n",
    "            #add created colunm names\n",
    "            df_merged4.columns = columns4\n",
    "            #save file\n",
    "            df_merged4.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part4.csv'.format(day,hour), index=False)\n",
    "#new minute (next_hour:00) data merge to fourth part of the dataset\n",
    "            try:\n",
    "                if((hour+1)!=24):\n",
    "                    with open(r'C:\\Melbourne_Escooter\\RealDataset\\August\\8_{}t{}_0.json'.format(day,(hour+1)),'r') as f:\n",
    "                        data = json.loads(f.read())\n",
    "                else:\n",
    "                    with open(r'C:\\Melbourne_Escooter\\RealDataset\\August\\8_{}t0_0.json'.format(day+1),'r') as f:\n",
    "                        data = json.loads(f.read())\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            df_normalized=pd.json_normalize(data, ['data', 'bikes'])\n",
    "            df_normalized_escooter=df_normalized[df_normalized['vehicle_type']=='scooter']\n",
    "            df_normalized_escooter.drop(['is_reserved', 'is_disabled','vehicle_type_id','pricing_plan_id','last_reported','vehicle_type'], axis=1, inplace=True)\n",
    "            df=pd.DataFrame(df_normalized_escooter)\n",
    "            df.columns = ['bike_id', 'lat_60', 'lon_60', 'range_60']\n",
    "            part4=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part4.csv'.format(day,hour))\n",
    "            df_4=part4.merge(df, how='outer', on='bike_id')\n",
    "            df_4.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part4.csv'.format(day,hour), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "701d6d7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: Passing 'suffixes' which cause duplicate columns {'lon_a', 'current_range_meters_a', 'lat_a'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:49: FutureWarning: Passing 'suffixes' which cause duplicate columns {'lon_a', 'current_range_meters_a', 'lat_a'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:77: FutureWarning: Passing 'suffixes' which cause duplicate columns {'lon_a', 'current_range_meters_a', 'lat_a'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:105: FutureWarning: Passing 'suffixes' which cause duplicate columns {'lon_a', 'current_range_meters_a', 'lat_a'} in the result is deprecated and will raise a MergeError in a future version.\n"
     ]
    }
   ],
   "source": [
    "dateSetPerHour(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0ba786",
   "metadata": {},
   "source": [
    "# Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a46580e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tripStarts(day):\n",
    "    for hour in range(0,24):\n",
    "#1-15 minutes dataset\n",
    "        try:\n",
    "            df1_=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part4.csv'.format(day,hour-1))\n",
    "            df1=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part1.csv'.format(day,hour))\n",
    "            df1__=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part2.csv'.format(day,hour))\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        started_list1=[]\n",
    "        if((df1['bike_id'].isin(df1_['bike_id']).any()) & (df1['bike_id'].isin(df1__['bike_id']).any())):\n",
    "            for i in range(2,14):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df1.columns:\n",
    "                        started=df1[(df1['lat_%s' %i].isnull()==False) & (df1['lat_%s' %(i+1)].isnull()==True)]\n",
    "                        started_list1.append(started)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        elif (df1['bike_id'].isin(df1_['bike_id']).any()):\n",
    "            for i in range(2,15):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df1.columns:\n",
    "                        started=df1[(df1['lat_%s' %i].isnull()==False) & (df1['lat_%s' %(i+1)].isnull()==True)]\n",
    "                        started_list1.append(started)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        elif (df1['bike_id'].isin(df1__['bike_id']).any()):\n",
    "            for i in range(1,14):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df1.columns:\n",
    "                        started=df1[(df1['lat_%s' %i].isnull()==False) & (df1['lat_%s' %(i+1)].isnull()==True)]\n",
    "                        started_list1.append(started)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        else:\n",
    "            for i in range(1,15):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df1.columns:\n",
    "                        started=df1[(df1['lat_%s' %i].isnull()==False) & (df1['lat_%s' %(i+1)].isnull()==True)]\n",
    "                        started_list1.append(started)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        started_df1= pd.DataFrame()\n",
    "        for item in started_list1:\n",
    "            started_df1 =  started_df1.append(item, ignore_index=True)\n",
    "        started_df1.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\Started\\{}_{}hour_part1.csv'.format(day,hour),index=False)\n",
    "#16-30 minutes dataset\n",
    "        try:\n",
    "            df2_=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part1.csv'.format(day,hour))\n",
    "            df2=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part2.csv'.format(day,hour))\n",
    "            df2__=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part3.csv'.format(day,hour))\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        started_list2=[]\n",
    "        if((df2['bike_id'].isin(df2_['bike_id']).any()) & (df2['bike_id'].isin(df2__['bike_id']).any())):\n",
    "            for i in range(17,29):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df2.columns:\n",
    "                        started=df2[(df2['lat_%s' %i].isnull()==False) & (df2['lat_%s' %(i+1)].isnull()==True)]\n",
    "                        started_list2.append(started)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        elif (df2['bike_id'].isin(df2_['bike_id']).any()):\n",
    "            for i in range(17,30):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df2.columns:\n",
    "                        started=df2[(df2['lat_%s' %i].isnull()==False) & (df2['lat_%s' %(i+1)].isnull()==True)]\n",
    "                        started_list2.append(started)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        elif(df2['bike_id'].isin(df2__['bike_id']).any()):\n",
    "            for i in range(16,29):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df2.columns:\n",
    "                        started=df2[(df2['lat_%s' %i].isnull()==False) & (df2['lat_%s' %(i+1)].isnull()==True)]\n",
    "                        started_list2.append(started)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        else:\n",
    "            for i in range(16,30):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df2.columns:\n",
    "                        started=df2[(df2['lat_%s' %i].isnull()==False) & (df2['lat_%s' %(i+1)].isnull()==True)]\n",
    "                        started_list2.append(started)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        started_df2= pd.DataFrame()\n",
    "        for item in started_list2:\n",
    "            started_df2 =  started_df2.append(item, ignore_index=True)\n",
    "        started_df2.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\Started\\{}_{}hour_part2.csv'.format(day,hour),index=False)\n",
    "#31-45 minutes dataset\n",
    "        try:\n",
    "            df3_=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part2.csv'.format(day,hour))\n",
    "            df3=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part3.csv'.format(day,hour))\n",
    "            df3__=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part4.csv'.format(day,hour))\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        started_list3=[]\n",
    "        if((df3['bike_id'].isin(df3_['bike_id']).any()) & (df3['bike_id'].isin(df3__['bike_id']).any())):\n",
    "            for i in range(32,44):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df3.columns:\n",
    "                        started=df3[(df3['lat_%s' %i].isnull()==False) & (df3['lat_%s' %(i+1)].isnull()==True)]\n",
    "                        started_list3.append(started)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        elif (df3['bike_id'].isin(df3_['bike_id']).any()):\n",
    "            for i in range(32,45):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df3.columns:\n",
    "                        started=df3[(df3['lat_%s' %i].isnull()==False) & (df3['lat_%s' %(i+1)].isnull()==True)]\n",
    "                        started_list3.append(started)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        elif(df3['bike_id'].isin(df3__['bike_id']).any()):\n",
    "            for i in range(31,44):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df3.columns:\n",
    "                        started=df3[(df3['lat_%s' %i].isnull()==False) & (df3['lat_%s' %(i+1)].isnull()==True)]\n",
    "                        started_list3.append(started)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        else:\n",
    "            for i in range(31,45):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df3.columns:\n",
    "                        started=df3[(df3['lat_%s' %i].isnull()==False) & (df3['lat_%s' %(i+1)].isnull()==True)]\n",
    "                        started_list3.append(started)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        started_df3= pd.DataFrame()\n",
    "        for item in started_list3:\n",
    "            started_df3 =  started_df3.append(item, ignore_index=True)\n",
    "        started_df3.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\Started\\{}_{}hour_part3.csv'.format(day,hour),index=False)\n",
    "#46-00 minutes dataset\n",
    "        try:\n",
    "            df4_=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part3.csv'.format(day,hour))\n",
    "            df4=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part4.csv'.format(day,hour))\n",
    "            df4__=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part1.csv'.format(day,hour+1))\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        started_list4=[]\n",
    "        if((df4['bike_id'].isin(df4_['bike_id']).any()) & (df4['bike_id'].isin(df4__['bike_id']).any())):\n",
    "            for i in range(47,59):\n",
    "                if 'lat_%s' %(i+1) in df4.columns:\n",
    "                    started=df4[(df4['lat_%s' %i].isnull()==False) & (df4['lat_%s' %(i+1)].isnull()==True)]\n",
    "                    started_list4.append(started)\n",
    "        elif (df4['bike_id'].isin(df4_['bike_id']).any()):\n",
    "            for i in range(47,60):\n",
    "                if 'lat_%s' %(i+1) in df4.columns:\n",
    "                    started=df4[(df4['lat_%s' %i].isnull()==False) & (df4['lat_%s' %(i+1)].isnull()==True)]\n",
    "                    started_list4.append(started)\n",
    "        elif(df4['bike_id'].isin(df4__['bike_id']).any()):\n",
    "             for i in range(46,59):\n",
    "                if 'lat_%s' %(i+1) in df4.columns:\n",
    "                    started=df4[(df4['lat_%s' %i].isnull()==False) & (df4['lat_%s' %(i+1)].isnull()==True)]\n",
    "                    started_list4.append(started)\n",
    "        else:\n",
    "            for i in range(46,60):\n",
    "                if 'lat_%s' %(i+1) in df4.columns:\n",
    "                    started=df4[(df4['lat_%s' %i].isnull()==False) & (df4['lat_%s' %(i+1)].isnull()==True)]\n",
    "                    started_list4.append(started)\n",
    "        started_df4= pd.DataFrame()\n",
    "        for item in started_list4:\n",
    "            started_df4 =  started_df4.append(item, ignore_index=True)\n",
    "        started_df4.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\Started\\{}_{}hour_part4.csv'.format(day,hour),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ad7aca72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values: hour 11_minute 39\n",
      "Missing Values: hour 13_minute 9\n",
      "Missing Values: hour 13_minute 38\n",
      "Missing Values: hour 13_minute 40\n",
      "Missing Values: hour 13_minute 43\n"
     ]
    }
   ],
   "source": [
    "tripStarts(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ca716abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filteredTripStarts(day):\n",
    "    for hour in range(0,24): \n",
    "        for partID in range(1,5):\n",
    "            try:\n",
    "                df= pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\Started\\{}_{}hour_part{}.csv'.format(day,hour,partID))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            column_num=len(df.columns)\n",
    "            value_df = df.iloc[:, 1:column_num]\n",
    "            new_list=[]\n",
    "            for i in range(len(df)):\n",
    "                row=value_df.loc[i]\n",
    "                new_row=row[row.isnull().shift(-3).fillna(False)]\n",
    "                new_list.append(new_row)\n",
    "\n",
    "            new_df= pd.DataFrame()\n",
    "            for item in new_list:\n",
    "                new_df =  new_df.append(item, ignore_index=True)\n",
    "            first_column= df.iloc[:, 0]\n",
    "            #print(new_df)\n",
    "            try:\n",
    "                df_all_cols = pd.concat([first_column,new_df], axis = 1)\n",
    "            except ValueError:\n",
    "                print(str(hour)+\"___\"+str(partID))\n",
    "                continue\n",
    "            df_all_cols.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\Started\\Filtered_starts\\{}_{}hour_part{}.csv'.format(day,hour,partID),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f2214ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredTripStarts(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "faa3b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDateTimeStarts(day):\n",
    "    for hour in range(0,24): \n",
    "        for partID in range(1,5):\n",
    "            try:\n",
    "                df= pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\Started\\Filtered_starts\\{}_{}hour_part{}.csv'.format(day,hour,partID))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            col_names=[]\n",
    "            datetime_list=[]\n",
    "            for col in df.columns:\n",
    "                col_names.append(col)\n",
    "            unique_col_names=col_names[1:][::3]\n",
    "            #take only lat_ like names\n",
    "            #uni_name_count=len(unique_col_names)\n",
    "            for names in unique_col_names:\n",
    "                minute=names.split(\"_\")[1]\n",
    "                date_time=str(day)+\":\"+str(hour)+\":\"+str(minute)\n",
    "                datetime_list.append(date_time)\n",
    "            #print(datetime_list)\n",
    "\n",
    "            for item in datetime_list:\n",
    "                col_common_name=item.split(\":\")[2]\n",
    "                item_index=datetime_list.index(item)\n",
    "                location =int(item_index+1)*4\n",
    "                column_name=\"date:time_\"+col_common_name\n",
    "                value=item\n",
    "                #df['date:time_1']=np.where(df['range_1'].notnull(),'value',np.NaN)\n",
    "                #print(location)\n",
    "                try:\n",
    "                    df.insert(location, column_name, value)\n",
    "                except ValueError:\n",
    "                    print(\"value error in \"+ str(day)+\"_\"+str(hour)+\"hour_part\"+str(partID)+\" file. ->>>>\"+str(value))\n",
    "                    continue\n",
    "                except IndexError:\n",
    "                    print(\"index error in \"+ str(day)+\"_\"+str(hour)+\"hour_part\"+str(partID)+\" file. ->>>>\"+str(value))\n",
    "                    continue\n",
    "                date_column_name=\"date:time_\"+col_common_name\n",
    "                range_column_name=\"range_\"+col_common_name\n",
    "                #df['date:time_1']=np.where(df['range_1'].notnull(),'value',np.NaN)\n",
    "                df[date_column_name]=np.where(df[range_column_name].notnull(),value,np.NaN)\n",
    "            df.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\Started\\Filtered_starts\\WithDateTime\\{}_{}hour_part{}_dateAdded.csv'.format(day,hour,partID),index=False)\n",
    "           # print(\"added!!!!!!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a814ba52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "addDateTimeStarts(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "18894a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_first_valid(series):\n",
    "    first_valid = series.first_valid_index()\n",
    "    return series.mask(series.index!=first_valid)\n",
    "\n",
    "def GetTripStartRecords(day):\n",
    "    for hour in range(0,24):\n",
    "        for fileID in range(1,5):\n",
    "            try:\n",
    "                df=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\Started\\Filtered_starts\\WithDateTime\\{}_{}hour_part{}_dateAdded.csv'.format(day,hour,fileID))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            df.drop_duplicates(inplace=True)\n",
    "            col_set=df.columns[1:]\n",
    "            #take column names except the first column\n",
    "            df_new = df.dropna(how='all', subset=col_set)\n",
    "            df_id=df_new.filter(like='bike_')\n",
    "            #df_lat.flags.allows_duplicate_labels = False\n",
    "            df_lat=df_new.filter(like='lat')\n",
    "            df_lat = df_lat.apply(lambda series: keep_first_valid(series), axis=1)\n",
    "            df_lat.columns = df_lat.columns.str.replace('lat_.*', 'latitude')\n",
    "            s = df_lat.stack()\n",
    "            df_lat_new = s.unstack()\n",
    "            df_lon=df_new.filter(like='lon')\n",
    "            df_lon = df_lon.apply(lambda series: keep_first_valid(series), axis=1)\n",
    "            df_lon.columns = df_lon.columns.str.replace('lon_.*', 'longitude')\n",
    "            s = df_lon.stack()\n",
    "            df_lon_new = s.unstack()\n",
    "            df_range=df_new.filter(like='range')\n",
    "            df_range = df_range.apply(lambda series: keep_first_valid(series), axis=1)\n",
    "            df_range.columns = df_range.columns.str.replace('range_.*', 'range')\n",
    "            s = df_range.stack()\n",
    "            df_range_new = s.unstack()\n",
    "            df_datetime=df_new.filter(like='date:time')\n",
    "            df_datetime = df_datetime.apply(lambda series: keep_first_valid(series), axis=1)\n",
    "            df_datetime.columns = df_datetime.columns.str.replace('date:time_.*', 'datetime')\n",
    "            s = df_datetime.stack()\n",
    "            df_datetime_new = s.unstack()\n",
    "            df_all = pd.concat([df_id,df_lat_new,df_lon_new,df_range_new,df_datetime_new],axis=1,sort=False)\n",
    "            df_all.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\FinalResults\\August\\Started\\{}_{}hour_part{}_final.csv'.format(day,hour,fileID),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "096c77e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    }
   ],
   "source": [
    "GetTripStartRecords(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "45ed3255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetHourlyStartTripRecords(day):\n",
    "    for hour in range(0,24):\n",
    "        df_list=[]\n",
    "        for fileID in range(1,5):\n",
    "            try:\n",
    "                df =pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\FinalResults\\August\\Started\\{}_{}hour_part{}_final.csv'.format(day,hour,fileID))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            df_list.append(df)\n",
    "        #print(df_list)\n",
    "        try:\n",
    "            all_df=pd.concat(df_list,ignore_index=True)\n",
    "        except ValueError:\n",
    "            print(\"value error -> \"+str(hour)+ \" hour\")\n",
    "            continue\n",
    "        #print(all_df)\n",
    "        all_df.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\FinalResults\\August\\Started\\Hourly\\{}_{}.csv'.format(day,hour), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "81ed2c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "GetHourlyStartTripRecords(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "02ee443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourlyStartTripCounts(day):\n",
    "    month=8\n",
    "    year=2022\n",
    "    tripDate = datetime.datetime(year, month, day)\n",
    "    file_name=tripDate.strftime(\"%b %d %Y\")\n",
    "    row_count_list=[]\n",
    "    hour_list=[]\n",
    "    for hour in range(0,24):\n",
    "        try:\n",
    "            df=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\FinalResults\\August\\Started\\Hourly\\{}_{}.csv'. format(day,hour))\n",
    "            row_count=len(df)\n",
    "            row_count_list.append(row_count)\n",
    "            hour_list.append(hour)\n",
    "        except FileNotFoundError:\n",
    "            print(\"No file found \"+str(hour))\n",
    "\n",
    "    s1=pd.Series(hour_list, name=\"hour\")\n",
    "    s2=pd.Series(row_count_list, name=\"count\")\n",
    "    hourly_count=pd.concat([s1,s2], axis=1)\n",
    "    #print(tripDate)\n",
    "    hourly_count.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\FinalResults\\August\\Started\\Hourly\\hourlyTripCount_{}.csv'.format(file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "aab8c2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "HourlyStartTripCounts(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c03a75",
   "metadata": {},
   "source": [
    "# Stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53f3bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tripStops(day):\n",
    "    for hour in range(0,24):  \n",
    "#1-15 minutes dataset\n",
    "        try:\n",
    "            df1_=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part4.csv'.format(day,hour-1))\n",
    "            df1=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part1.csv'.format(day,hour))\n",
    "            df1__=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part1.csv'.format(day,hour))\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        stopped_list1=[]\n",
    "        if((df1['bike_id'].isin(df1_['bike_id']).any()) & (df1['bike_id'].isin(df1__['bike_id']).any())):\n",
    "            for i in range(2,14):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df1.columns:\n",
    "                        stopped=df1[(df1['lat_%s' %i].isnull()==True) & (df1['lat_%s' %(i+1)].isnull()==False)]\n",
    "                        stopped_list1.append(stopped)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        elif (df1['bike_id'].isin(df1_['bike_id']).any()):\n",
    "            for i in range(2,15):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df1.columns:\n",
    "                        stopped=df1[(df1['lat_%s' %i].isnull()==True) & (df1['lat_%s' %(i+1)].isnull()==False)]\n",
    "                        stopped_list1.append(stopped)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        elif (df1['bike_id'].isin(df1__['bike_id']).any()):\n",
    "            for i in range(1,14):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df1.columns:\n",
    "                        stopped=df1[(df1['lat_%s' %i].isnull()==True) & (df1['lat_%s' %(i+1)].isnull()==False)]\n",
    "                        stopped_list1.append(stopped)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        else:\n",
    "            for i in range(1,15):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df1.columns:\n",
    "                        stopped=df1[(df1['lat_%s' %i].isnull()==True) & (df1['lat_%s' %(i+1)].isnull()==False)]\n",
    "                        stopped_list1.append(stopped)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        stopped_df1= pd.DataFrame()\n",
    "        for item in stopped_list1:\n",
    "            stopped_df1 =  stopped_df1.append(item, ignore_index=True)\n",
    "        stopped_df1.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\Stopped\\{}_{}hour_part1.csv'.format(day,hour),index=False)\n",
    "#16-30 minutes dataset\n",
    "        try:\n",
    "            df2_=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part1.csv'.format(day,hour))\n",
    "            df2=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part2.csv'.format(day,hour))\n",
    "            df2__=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part3.csv'.format(day,hour))\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        stopped_list2=[]\n",
    "        if((df2['bike_id'].isin(df2_['bike_id']).any()) & (df2['bike_id'].isin(df2__['bike_id']).any())):\n",
    "            for i in range(17,29):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df2.columns:\n",
    "                        stopped=df2[(df2['lat_%s' %i].isnull()==True) & (df2['lat_%s' %(i+1)].isnull()==False)]\n",
    "                        stopped_list2.append(stopped)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        elif (df2['bike_id'].isin(df2_['bike_id']).any()):\n",
    "            for i in range(17,30):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df2.columns:\n",
    "                        stopped=df2[(df2['lat_%s' %i].isnull()==True) & (df2['lat_%s' %(i+1)].isnull()==False)]\n",
    "                        stopped_list2.append(stopped)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        elif (df2['bike_id'].isin(df2__['bike_id']).any()):\n",
    "            for i in range(16,29):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df2.columns:\n",
    "                        stopped=df2[(df2['lat_%s' %i].isnull()==True) & (df2['lat_%s' %(i+1)].isnull()==False)]\n",
    "                        stopped_list2.append(stopped)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        else:\n",
    "            for i in range(16,30):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df2.columns:\n",
    "                        stopped=df2[(df2['lat_%s' %i].isnull()==True) & (df2['lat_%s' %(i+1)].isnull()==False)]\n",
    "                        stopped_list2.append(stopped)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        stopped_df2= pd.DataFrame()\n",
    "        for item in stopped_list2:\n",
    "            stopped_df2 =  stopped_df2.append(item, ignore_index=True)\n",
    "        stopped_df2.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\Stopped\\{}_{}hour_part2.csv'.format(day,hour),index=False)\n",
    "#31-45 minutes dataset\n",
    "        try:\n",
    "            df3_=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part2.csv'.format(day,hour))\n",
    "            df3=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part3.csv'.format(day,hour))\n",
    "            df3__=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part4.csv'.format(day,hour))\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        stopped_list3=[]\n",
    "        if((df3['bike_id'].isin(df3_['bike_id']).any()) & (df3['bike_id'].isin(df3__['bike_id']).any())):\n",
    "            for i in range(32,44):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df3.columns:\n",
    "                        stopped=df3[(df3['lat_%s' %i].isnull()==True) & (df3['lat_%s' %(i+1)].isnull()==False)]\n",
    "                        stopped_list3.append(stopped)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        elif (df3['bike_id'].isin(df3_['bike_id']).any()):\n",
    "            for i in range(32,45):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df3.columns:\n",
    "                        stopped=df3[(df3['lat_%s' %i].isnull()==True) & (df3['lat_%s' %(i+1)].isnull()==False)]\n",
    "                        stopped_list3.append(stopped)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        elif (df3['bike_id'].isin(df3__['bike_id']).any()):\n",
    "            for i in range(31,44):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df3.columns:\n",
    "                        stopped=df3[(df3['lat_%s' %i].isnull()==True) & (df3['lat_%s' %(i+1)].isnull()==False)]\n",
    "                        stopped_list3.append(stopped)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        else:\n",
    "            for i in range(31,45):\n",
    "                try:\n",
    "                    if 'lat_%s' %(i+1) in df3.columns:\n",
    "                        stopped=df3[(df3['lat_%s' %i].isnull()==True) & (df3['lat_%s' %(i+1)].isnull()==False)]\n",
    "                        stopped_list3.append(stopped)\n",
    "                except KeyError:\n",
    "                    print(\"Missing Values: hour \"+str(hour)+\"_minute \"+str(i))\n",
    "        stopped_df3= pd.DataFrame()\n",
    "        for item in stopped_list3:\n",
    "            stopped_df3 =  stopped_df3.append(item, ignore_index=True)\n",
    "        stopped_df3.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\Stopped\\{}_{}hour_part3.csv'.format(day,hour),index=False)\n",
    "#46-00 minutes dataset\n",
    "        try:\n",
    "            df4_=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part3.csv'.format(day,hour))\n",
    "            df4=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part4.csv'.format(day,hour))\n",
    "            df4__=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\{}_{}hour_part1.csv'.format(day,hour+1))\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        stopped_list4=[]\n",
    "        if((df4['bike_id'].isin(df4_['bike_id']).any()) & (df4['bike_id'].isin(df4__['bike_id']).any())):\n",
    "            for i in range(47,59):\n",
    "                if 'lat_%s' %(i+1) in df4.columns:\n",
    "                    stopped=df4[(df4['lat_%s' %i].isnull()==True) & (df4['lat_%s' %(i+1)].isnull()==False)]\n",
    "                    stopped_list4.append(stopped)\n",
    "        elif (df4['bike_id'].isin(df4_['bike_id']).any()):\n",
    "            for i in range(47,60):\n",
    "                if 'lat_%s' %(i+1) in df4.columns:\n",
    "                    stopped=df4[(df4['lat_%s' %i].isnull()==True) & (df4['lat_%s' %(i+1)].isnull()==False)]\n",
    "                    stopped_list4.append(stopped)\n",
    "        elif (df4['bike_id'].isin(df4__['bike_id']).any()):\n",
    "            for i in range(46,59):\n",
    "                if 'lat_%s' %(i+1) in df4.columns:\n",
    "                    stopped=df4[(df4['lat_%s' %i].isnull()==True) & (df4['lat_%s' %(i+1)].isnull()==False)]\n",
    "                    stopped_list4.append(stopped)\n",
    "        else:\n",
    "            for i in range(46,60):\n",
    "                if 'lat_%s' %(i+1) in df4.columns:\n",
    "                    stopped=df4[(df4['lat_%s' %i].isnull()==True) & (df4['lat_%s' %(i+1)].isnull()==False)]\n",
    "                    stopped_list4.append(stopped)\n",
    "        stopped_df4= pd.DataFrame()\n",
    "        for item in stopped_list4:\n",
    "            stopped_df4 =  stopped_df4.append(item, ignore_index=True)\n",
    "        stopped_df4.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\Stopped\\{}_{}hour_part4.csv'.format(day,hour),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69638d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values: hour 12_minute 33\n",
      "Missing Values: hour 12_minute 37\n",
      "Missing Values: hour 16_minute 2\n",
      "Missing Values: hour 16_minute 9\n"
     ]
    }
   ],
   "source": [
    "tripStops(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ecb11d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filteredTripStops(day):\n",
    "    for hour in range(0,24): \n",
    "        for fileID in range(1,5):\n",
    "            try:\n",
    "                df= pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\Stopped\\{}_{}hour_part{}.csv'.format(day,hour,fileID))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            Column_num=len(df.columns)\n",
    "            value_df = df.iloc[:, 1:Column_num]\n",
    "            new_list=[]\n",
    "\n",
    "            for i in range((len(df)-1),-1,-1):\n",
    "                row=value_df.loc[i]\n",
    "                new_row=row[row.isnull().shift(3).fillna(False)]\n",
    "                new_list.append(new_row)\n",
    "\n",
    "            new_df= pd.DataFrame()\n",
    "            for item in  new_list:\n",
    "                new_df =  new_df.append(item, ignore_index=True)\n",
    "\n",
    "            first_column= df.iloc[:, 0]\n",
    "            rvs_first_column =first_column[::-1].reset_index(drop=True)\n",
    "            df_all_cols = pd.concat([rvs_first_column,new_df], axis = 1)\n",
    "            df_all_cols.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\Stopped\\Filtered_stops\\{}_{}hour_part{}.csv'.format(day,hour,fileID),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a687c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredTripStops(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2d8d37cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDateTimeStops(day):\n",
    "    for hour in range(0,24): \n",
    "        for partID in range(1,5):\n",
    "            try:\n",
    "                df= pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\Stopped\\Filtered_stops\\{}_{}hour_part{}.csv'.format(day,hour,partID))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            col_names=[]\n",
    "            datetime_list=[]\n",
    "            for col in df.columns:\n",
    "                col_names.append(col)\n",
    "            unique_col_names=col_names[1:][::3]\n",
    "            #take only lat_ like names\n",
    "            #uni_name_count=len(unique_col_names)\n",
    "            for names in unique_col_names:\n",
    "                minute=names.split(\"_\")[1]\n",
    "                date_time=str(day)+\":\"+str(hour)+\":\"+str(minute)\n",
    "                datetime_list.append(date_time)\n",
    "            #print(datetime_list)\n",
    "\n",
    "            for item in datetime_list:\n",
    "                col_common_name=item.split(\":\")[2]\n",
    "                item_index=datetime_list.index(item)\n",
    "                location =int(item_index+1)*4\n",
    "                column_name=\"date:time_\"+col_common_name\n",
    "                value=item\n",
    "    #check type of value and make it string, not date time\n",
    "                #df['date:time_1']=np.where(df['range_1'].notnull(),'value',np.NaN)\n",
    "                #print(location)\n",
    "                try:\n",
    "                    df.insert(location, column_name, value)\n",
    "                except ValueError:\n",
    "                    print(\"value error in \"+ str(day)+\"_\"+str(hour)+\"hour_part\"+str(partID)+\" file. ->>>>\"+str(value))\n",
    "                    continue\n",
    "                except IndexError:\n",
    "                    print(\"index error in \"+ str(day)+\"_\"+str(hour)+\"hour_part\"+str(partID)+\" file. ->>>>\"+str(value))\n",
    "                    continue\n",
    "                date_column_name=\"date:time_\"+col_common_name\n",
    "                range_column_name=\"range_\"+col_common_name\n",
    "                #df['date:time_1']=np.where(df['range_1'].notnull(),'value',np.NaN)\n",
    "                df[date_column_name]=np.where(df[range_column_name].notnull(),value,np.NaN)\n",
    "            df.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\Stopped\\Filtered_stops\\WithDateTime\\{}_{}hour_part{}_dateAdded.csv'.format(day,hour,partID),index=False)\n",
    "           # print(\"added!!!!!!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "02a7b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "addDateTimeStops(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bc920299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_first_valid(series):\n",
    "    first_valid = series.first_valid_index()\n",
    "    return series.mask(series.index!=first_valid)\n",
    "\n",
    "def GetTripStopRecords(day):\n",
    "    for hour in range(0,24):\n",
    "        for fileID in range(1,5):\n",
    "            try:\n",
    "                df=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\August\\Stopped\\Filtered_stops\\WithDateTime\\{}_{}hour_part{}_dateAdded.csv'.format(day,hour,fileID))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            df.drop_duplicates(inplace=True)\n",
    "            col_set=df.columns[1:]\n",
    "            #take column names except the first column\n",
    "            df_new = df.dropna(how='all', subset=col_set)\n",
    "            df_id=df_new.filter(like='bike_')\n",
    "            #df_lat.flags.allows_duplicate_labels = False\n",
    "            df_lat=df_new.filter(like='lat')\n",
    "            df_lat = df_lat.apply(lambda series: keep_first_valid(series), axis=1)\n",
    "            df_lat.columns = df_lat.columns.str.replace('lat_.*', 'latitude')\n",
    "            s = df_lat.stack()\n",
    "            df_lat_new = s.unstack()\n",
    "            df_lon=df_new.filter(like='lon')\n",
    "            df_lon = df_lon.apply(lambda series: keep_first_valid(series), axis=1)\n",
    "            df_lon.columns = df_lon.columns.str.replace('lon_.*', 'longitude')\n",
    "            s = df_lon.stack()\n",
    "            df_lon_new = s.unstack()\n",
    "            df_range=df_new.filter(like='range')\n",
    "            df_range = df_range.apply(lambda series: keep_first_valid(series), axis=1)\n",
    "            df_range.columns = df_range.columns.str.replace('range_.*', 'range')\n",
    "            s = df_range.stack()\n",
    "            df_range_new = s.unstack()\n",
    "            df_datetime=df_new.filter(like='date:time')\n",
    "            df_datetime = df_datetime.apply(lambda series: keep_first_valid(series), axis=1)\n",
    "            df_datetime.columns = df_datetime.columns.str.replace('date:time_.*', 'datetime')\n",
    "            s = df_datetime.stack()\n",
    "            df_datetime_new = s.unstack()\n",
    "            df_all = pd.concat([df_id,df_lat_new,df_lon_new,df_range_new,df_datetime_new],axis=1,sort=False)\n",
    "            df_all.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\FinalResults\\August\\Stopped\\{}_{}hour_part{}_final.csv'.format(day,hour,fileID),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8751ef72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    }
   ],
   "source": [
    "GetTripStopRecords(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e7161949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetHourlyStopTripRecords(day):\n",
    "    for hour in range(0,24):\n",
    "        df_list=[]\n",
    "        for fileID in range(1,5):\n",
    "            try:\n",
    "                df =pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\FinalResults\\August\\Stopped\\{}_{}hour_part{}_final.csv'.format(day,hour,fileID))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            df_list.append(df)\n",
    "        #print(df_list)\n",
    "        try:\n",
    "            all_df=pd.concat(df_list,ignore_index=True)\n",
    "        except ValueError:\n",
    "            print(\"value error -> \"+str(hour)+ \" hour\")\n",
    "            continue\n",
    "        #print(all_df)\n",
    "        all_df.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\FinalResults\\August\\Stopped\\Hourly\\{}_{}.csv'.format(day,hour), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "61177559",
   "metadata": {},
   "outputs": [],
   "source": [
    "GetHourlyStopTripRecords(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c902b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourlyStopTripCounts(day):\n",
    "    month=8\n",
    "    year=2022\n",
    "    tripDate = datetime.datetime(year, month, day)\n",
    "    file_name=tripDate.strftime(\"%b %d %Y\")\n",
    "    row_count_list=[]\n",
    "    hour_list=[]\n",
    "    for hour in range(0,24):\n",
    "        try:\n",
    "            df=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\FinalResults\\August\\Stopped\\Hourly\\{}_{}.csv'. format(day,hour))\n",
    "            row_count=len(df)\n",
    "            row_count_list.append(row_count)\n",
    "            hour_list.append(hour)\n",
    "        except FileNotFoundError:\n",
    "            print(\"No file found \"+str(hour))\n",
    "\n",
    "    s1=pd.Series(hour_list, name=\"hour\")\n",
    "    s2=pd.Series(row_count_list, name=\"count\")\n",
    "    hourly_count=pd.concat([s1,s2], axis=1)\n",
    "    #print(tripDate)\n",
    "    hourly_count.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\FinalResults\\August\\Stopped\\Hourly\\hourlyTripCount_{}.csv'.format(file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "497a7427",
   "metadata": {},
   "outputs": [],
   "source": [
    "HourlyStopTripCounts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc14ca6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
