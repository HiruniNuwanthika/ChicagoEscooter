{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0135ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateSetPerHour(day):\n",
    "    for hour in range(0,24):\n",
    "#first part of the data\n",
    "        data_frames1 = []\n",
    "        files1=[]\n",
    "        for file_id in range(1,16):\n",
    "            try:\n",
    "                with open(r'C:\\Melbourne_Escooter\\RealDataset\\July\\7_{}t{}_{}.json'.format(day,hour,file_id),'r') as f:\n",
    "                    data = json.loads(f.read())\n",
    "                df_normalized=pd.json_normalize(data, ['data', 'bikes'])\n",
    "                df_normalized_escooter=df_normalized[df_normalized['vehicle_type']=='scooter']\n",
    "                df_normalized_escooter.drop(['is_reserved', 'is_disabled','vehicle_type_id','pricing_plan_id','last_reported','vehicle_type'], axis=1, inplace=True)\n",
    "                df=pd.DataFrame(df_normalized_escooter)\n",
    "                data_frames1.append(df)\n",
    "                files1.append(file_id)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        if(len(data_frames1)!=0):\n",
    "            df_merged1 = reduce(lambda  left,right: pd.merge(left,right,on=['bike_id'],suffixes=('_a', '_b'),\n",
    "                                                   how='outer'), data_frames1)\n",
    "            #create dynamic column names\n",
    "            columns1=['bike_id']\n",
    "            for i in files1:\n",
    "                columns1.append('lat_%s' %i)\n",
    "                columns1.append('lon_%s' %i)\n",
    "                columns1.append('range_%s' %i)\n",
    "            #print(columns)\n",
    "            #add created colunm names\n",
    "            df_merged1.columns = columns1\n",
    "            #save file\n",
    "            df_merged1.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\{}_{}hour_part1.csv'.format(day,hour), index=False)\n",
    "#second part of the data\n",
    "        data_frames2= []\n",
    "        files2=[]\n",
    "        for file_id in range(16,31):\n",
    "            try:\n",
    "                with open(r'C:\\Melbourne_Escooter\\RealDataset\\July\\7_{}t{}_{}.json'.format(day,hour,file_id),'r') as f:\n",
    "                    data = json.loads(f.read())\n",
    "                df_normalized=pd.json_normalize(data, ['data', 'bikes'])\n",
    "                df_normalized_escooter=df_normalized[df_normalized['vehicle_type']=='scooter']\n",
    "                df_normalized_escooter.drop(['is_reserved', 'is_disabled','vehicle_type_id','pricing_plan_id','last_reported','vehicle_type'], axis=1, inplace=True)\n",
    "                df=pd.DataFrame(df_normalized_escooter)\n",
    "                data_frames2.append(df)\n",
    "                files2.append(file_id)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        if(len(data_frames2)!=0):\n",
    "            df_merged2 = reduce(lambda  left,right: pd.merge(left,right,on=['bike_id'],suffixes=('_a', '_b'),\n",
    "                                                   how='outer'), data_frames2)\n",
    "            #create dynamic column names\n",
    "            columns2=['bike_id']\n",
    "            for i in files2:\n",
    "                columns2.append('lat_%s' %i)\n",
    "                columns2.append('lon_%s' %i)\n",
    "                columns2.append('range_%s' %i)\n",
    "            #add created colunm names\n",
    "            df_merged2.columns = columns2\n",
    "            #save file\n",
    "            df_merged2.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\{}_{}hour_part2.csv'.format(day,hour), index=False)\n",
    "#Third part of the dataset\n",
    "        data_frames3= []\n",
    "        files3=[]\n",
    "        for file_id in range(31,46):\n",
    "            try:\n",
    "                with open(r'C:\\Melbourne_Escooter\\RealDataset\\July\\7_{}t{}_{}.json'.format(day,hour,file_id),'r') as f:\n",
    "                    data = json.loads(f.read())\n",
    "                df_normalized=pd.json_normalize(data, ['data', 'bikes'])\n",
    "                df_normalized_escooter=df_normalized[df_normalized['vehicle_type']=='scooter']\n",
    "                df_normalized_escooter.drop(['is_reserved', 'is_disabled','vehicle_type_id','pricing_plan_id','last_reported','vehicle_type'], axis=1, inplace=True)\n",
    "                df=pd.DataFrame(df_normalized_escooter)\n",
    "                data_frames3.append(df)\n",
    "                files3.append(file_id)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        if(len(data_frames3)!=0):\n",
    "            df_merged3 = reduce(lambda  left,right: pd.merge(left,right,on=['bike_id'],suffixes=('_a', '_b'),\n",
    "                                                   how='outer'), data_frames3)\n",
    "            #create dynamic column names\n",
    "            columns3=['bike_id']\n",
    "            for i in files3:\n",
    "                columns3.append('lat_%s' %i)\n",
    "                columns3.append('lon_%s' %i)\n",
    "                columns3.append('range_%s' %i)\n",
    "            #add created colunm names\n",
    "            df_merged3.columns = columns3\n",
    "            #save file\n",
    "            df_merged3.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\{}_{}hour_part3.csv'.format(day,hour), index=False)\n",
    "#fourth part of the dataset\n",
    "        data_frames4= []\n",
    "        files4=[]\n",
    "        for file_id in range(46,60):\n",
    "            try:\n",
    "                with open(r'C:\\Melbourne_Escooter\\RealDataset\\July\\7_{}t{}_{}.json'.format(day,hour,file_id),'r') as f:\n",
    "                    data = json.loads(f.read())\n",
    "                df_normalized=pd.json_normalize(data, ['data', 'bikes'])\n",
    "                df_normalized_escooter=df_normalized[df_normalized['vehicle_type']=='scooter']\n",
    "                df_normalized_escooter.drop(['is_reserved', 'is_disabled','vehicle_type_id','pricing_plan_id','last_reported','vehicle_type'], axis=1, inplace=True)\n",
    "                df=pd.DataFrame(df_normalized_escooter)\n",
    "                data_frames4.append(df)\n",
    "                files4.append(file_id)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        if(len(data_frames4)!=0):\n",
    "            df_merged4 = reduce(lambda  left,right: pd.merge(left,right,on=['bike_id'],suffixes=('_a', '_b'),\n",
    "                                                   how='outer'), data_frames4)\n",
    "            #create dynamic column names\n",
    "            columns4=['bike_id']\n",
    "            for i in files4:\n",
    "                columns4.append('lat_%s' %i)\n",
    "                columns4.append('lon_%s' %i)\n",
    "                columns4.append('range_%s' %i)\n",
    "            #add created colunm names\n",
    "            df_merged4.columns = columns4\n",
    "            #save file\n",
    "            df_merged4.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\{}_{}hour_part4.csv'.format(day,hour), index=False)\n",
    "#new minute (next_hour:00) data merge to fourth part of the dataset\n",
    "            try:\n",
    "                if((hour+1)!=24):\n",
    "                    with open(r'C:\\Melbourne_Escooter\\RealDataset\\July\\7_{}t{}_0.json'.format(day,(hour+1)),'r') as f:\n",
    "                        data = json.loads(f.read())\n",
    "                else:\n",
    "                    with open(r'C:\\Melbourne_Escooter\\RealDataset\\July\\7_{}t0_0.json'.format(day+1),'r') as f:\n",
    "                        data = json.loads(f.read())\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            df_normalized=pd.json_normalize(data, ['data', 'bikes'])\n",
    "            df_normalized_escooter=df_normalized[df_normalized['vehicle_type']=='scooter']\n",
    "            df_normalized_escooter.drop(['is_reserved', 'is_disabled','vehicle_type_id','pricing_plan_id','last_reported','vehicle_type'], axis=1, inplace=True)\n",
    "            df=pd.DataFrame(df_normalized_escooter)\n",
    "            df.columns = ['bike_id', 'lat_60', 'lon_60', 'range_60']\n",
    "            part4=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\{}_{}hour_part4.csv'.format(day,hour))\n",
    "            df_4=part4.merge(df, how='outer', on='bike_id')\n",
    "            df_4.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\{}_{}hour_part4.csv'.format(day,hour), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fe95cb",
   "metadata": {},
   "source": [
    "# Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b244107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tripStarts(day):\n",
    "    for hour in range(0,24):  \n",
    "#1-15 minutes dataset\n",
    "        try:\n",
    "            df1=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\{}_{}hour_part1.csv'.format(day,hour))\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        started_list1=[]\n",
    "        for i in range(1,16):\n",
    "            if 'lat_%s' %(i+1) in df1.columns:\n",
    "                started=df1[(df1['lat_%s' %i].isnull()==False) & (df1['lat_%s' %(i+1)].isnull()==True)]\n",
    "                started_list1.append(started)\n",
    "        started_df1= pd.DataFrame()\n",
    "        for item in started_list1:\n",
    "            started_df1 =  started_df1.append(item, ignore_index=True)\n",
    "        started_df1.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\Started\\{}_{}hour_part1.csv'.format(day,hour),index=False)\n",
    "#16-30 minutes dataset\n",
    "        try:\n",
    "            df2=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\{}_{}hour_part2.csv'.format(day,hour))\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        started_list2=[]\n",
    "        for i in range(16,31):\n",
    "            if 'lat_%s' %(i+1) in df2.columns:\n",
    "                started=df2[(df2['lat_%s' %i].isnull()==True) & (df2['lat_%s' %(i+1)].isnull()==False)]\n",
    "                started_list2.append(started)\n",
    "        started_df2= pd.DataFrame()\n",
    "        for item in started_list2:\n",
    "            started_df2 =  started_df2.append(item, ignore_index=True)\n",
    "        started_df2.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\Started\\{}_{}hour_part2.csv'.format(day,hour),index=False)\n",
    "#31-45 minutes dataset\n",
    "        try:\n",
    "            df3=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\{}_{}hour_part3.csv'.format(day,hour))\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        started_list3=[]\n",
    "        for i in range(31,46):\n",
    "            if 'lat_%s' %(i+1) in df3.columns:\n",
    "                started=df3[(df3['lat_%s' %i].isnull()==True) & (df3['lat_%s' %(i+1)].isnull()==False)]\n",
    "                started_list3.append(started)\n",
    "        started_df3= pd.DataFrame()\n",
    "        for item in started_list3:\n",
    "            started_df3 =  started_df3.append(item, ignore_index=True)\n",
    "        started_df3.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\Started\\{}_{}hour_part3.csv'.format(day,hour),index=False)\n",
    "#46-00 minutes dataset\n",
    "        try:\n",
    "            df4=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\{}_{}hour_part4.csv'.format(day,hour))\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        started_list4=[]\n",
    "        for i in range(46,60):\n",
    "            if 'lat_%s' %(i+1) in df4.columns:\n",
    "                started=df4[(df4['lat_%s' %i].isnull()==True) & (df4['lat_%s' %(i+1)].isnull()==False)]\n",
    "                started_list4.append(started)\n",
    "        started_df4= pd.DataFrame()\n",
    "        for item in started_list4:\n",
    "            started_df4 =  started_df4.append(item, ignore_index=True)\n",
    "        started_df4.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\Started\\{}_{}hour_part4.csv'.format(day,hour),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a51d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filteredTripStarts(day):\n",
    "    for hour in range(0,24): \n",
    "        for partID in range(1,5):\n",
    "            try:\n",
    "                df= pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\Started\\{}_{}hour_part{}.csv'.format(day,hour,partID))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            column_num=len(df.columns)\n",
    "            value_df = df.iloc[:, 1:column_num]\n",
    "            new_list=[]\n",
    "            for i in range(len(df)):\n",
    "                row=value_df.loc[i]\n",
    "                new_row=row[row.isnull().shift(-3).fillna(False)]\n",
    "                new_list.append(new_row)\n",
    "\n",
    "            new_df= pd.DataFrame()\n",
    "            for item in new_list:\n",
    "                new_df =  new_df.append(item, ignore_index=True)\n",
    "            first_column= df.iloc[:, 0]\n",
    "            df_all_cols = pd.concat([first_column,new_df], axis = 1)\n",
    "            df_all_cols.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\Started\\Filtered_starts\\{}_{}hour_part{}.csv'.format(day,hour,partID),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e89cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDateTime(day):\n",
    "    for hour in range(0,24): \n",
    "        for partID in range(1,5):\n",
    "            try:\n",
    "                df= pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\Started\\Filtered_starts\\{}_{}hour_part{}.csv'.format(day,hour,partID))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            col_names=[]\n",
    "            datetime_list=[]\n",
    "            for col in df.columns:\n",
    "                col_names.append(col)\n",
    "            unique_col_names=col_names[1:][::3]\n",
    "            #take only lat_ like names\n",
    "            #uni_name_count=len(unique_col_names)\n",
    "            for names in unique_col_names:\n",
    "                minute=names.split(\"_\")[1]\n",
    "                date_time=str(day)+\":\"+str(hour)+\":\"+str(minute)\n",
    "                datetime_list.append(date_time)\n",
    "            #print(datetime_list)\n",
    "\n",
    "            for item in datetime_list:\n",
    "                col_common_name=item.split(\":\")[2]\n",
    "                item_index=datetime_list.index(item)\n",
    "                location =int(item_index+1)*4\n",
    "                column_name=\"date:time_\"+col_common_name\n",
    "                value=item\n",
    "                #df['date:time_1']=np.where(df['range_1'].notnull(),'value',np.NaN)\n",
    "                #print(location)\n",
    "                try:\n",
    "                    df.insert(location, column_name, value)\n",
    "                except ValueError:\n",
    "                    print(\"value error in \"+ str(day)+\"_\"+str(hour)+\"hour_part\"+str(partID)+\" file. ->>>>\"+str(value))\n",
    "                    continue\n",
    "                except IndexError:\n",
    "                    print(\"index error in \"+ str(day)+\"_\"+str(hour)+\"hour_part\"+str(partID)+\" file. ->>>>\"+str(value))\n",
    "                    continue\n",
    "                date_column_name=\"date:time_\"+col_common_name\n",
    "                range_column_name=\"range_\"+col_common_name\n",
    "                #df['date:time_1']=np.where(df['range_1'].notnull(),'value',np.NaN)\n",
    "                df[date_column_name]=np.where(df[range_column_name].notnull(),value,np.NaN)\n",
    "            df.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\Started\\Filtered_starts\\WithDateTime\\{}_{}hour_part{}_dateAdded.csv'.format(day,hour,partID),index=False)\n",
    "           # print(\"added!!!!!!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a7656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_first_valid(series):\n",
    "    first_valid = series.first_valid_index()\n",
    "    return series.mask(series.index!=first_valid)\n",
    "\n",
    "def GetTripStartRecords(day):\n",
    "    for hour in range(0,24):\n",
    "        for fileID in range(1,5):\n",
    "            try:\n",
    "                df=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\Started\\Filtered_starts\\WithDateTime\\{}_{}hour_part{}_dateAdded.csv'.format(day,hour,fileID))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            df.drop_duplicates(inplace=True)\n",
    "            col_set=df.columns[1:]\n",
    "            #take column names except the first column\n",
    "            df_new = df.dropna(how='all', subset=col_set)\n",
    "            df_id=df_new.filter(like='bike_')\n",
    "            #df_lat.flags.allows_duplicate_labels = False\n",
    "            df_lat=df_new.filter(like='lat')\n",
    "            df_lat = df_lat.apply(lambda series: keep_first_valid(series), axis=1)\n",
    "            df_lat.columns = df_lat.columns.str.replace('lat_.*', 'latitude')\n",
    "            s = df_lat.stack()\n",
    "            df_lat_new = s.unstack()\n",
    "            df_lon=df_new.filter(like='lon')\n",
    "            df_lon = df_lon.apply(lambda series: keep_first_valid(series), axis=1)\n",
    "            df_lon.columns = df_lon.columns.str.replace('lon_.*', 'longitude')\n",
    "            s = df_lon.stack()\n",
    "            df_lon_new = s.unstack()\n",
    "            df_range=df_new.filter(like='range')\n",
    "            df_range = df_range.apply(lambda series: keep_first_valid(series), axis=1)\n",
    "            df_range.columns = df_range.columns.str.replace('range_.*', 'range')\n",
    "            s = df_range.stack()\n",
    "            df_range_new = s.unstack()\n",
    "            df_datetime=df_new.filter(like='date:time')\n",
    "            df_datetime = df_datetime.apply(lambda series: keep_first_valid(series), axis=1)\n",
    "            df_datetime.columns = df_datetime.columns.str.replace('date:time_.*', 'datetime')\n",
    "            s = df_datetime.stack()\n",
    "            df_datetime_new = s.unstack()\n",
    "            df_all = pd.concat([df_id,df_lat_new,df_lon_new,df_range_new,df_datetime_new],axis=1,sort=False)\n",
    "            df_all.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\FinalResults\\July\\Started\\{}_{}hour_part{}_final.csv'.format(day,hour,fileID),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9853a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetHourlyTripRecords(day):\n",
    "    for hour in range(0,24):\n",
    "        df_list=[]\n",
    "        for fileID in range(1,5):\n",
    "            try:\n",
    "                df =pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\FinalResults\\July\\Started\\{}_{}hour_part{}_final.csv'.format(day,hour,fileID))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            df_list.append(df)\n",
    "        #print(df_list)\n",
    "        try:\n",
    "            all_df=pd.concat(df_list,ignore_index=True)\n",
    "        except ValueError:\n",
    "            print(\"value error -> \"+str(hour)+ \" hour\")\n",
    "            continue\n",
    "        #print(all_df)\n",
    "        all_df.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\FinalResults\\July\\Started\\Hourly\\{}_{}.csv'.format(day,hour), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a6a685",
   "metadata": {},
   "source": [
    "# Stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1cd90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tripStops(day):\n",
    "    for hour in range(0,24):  \n",
    "#1-15 minutes dataset\n",
    "        try:\n",
    "            df1=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\{}_{}hour_part1.csv'.format(day,hour))\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        stopped_list1=[]\n",
    "        for i in range(1,16):\n",
    "            if 'lat_%s' %(i+1) in df1.columns:\n",
    "                stopped=df1[(df1['lat_%s' %i].isnull()==True) & (df1['lat_%s' %(i+1)].isnull()==False)]\n",
    "                stopped_list1.append(stopped)\n",
    "        stopped_df1= pd.DataFrame()\n",
    "        for item in stopped_list1:\n",
    "            stopped_df1 =  stopped_df1.append(item, ignore_index=True)\n",
    "        stopped_df1.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\Stopped\\{}_{}hour_part1.csv'.format(day,hour),index=False)\n",
    "#16-30 minutes dataset\n",
    "        try:\n",
    "            df2=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\{}_{}hour_part2.csv'.format(day,hour))\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        stopped_list2=[]\n",
    "        for i in range(16,31):\n",
    "            if 'lat_%s' %(i+1) in df2.columns:\n",
    "                stopped=df2[(df2['lat_%s' %i].isnull()==True) & (df2['lat_%s' %(i+1)].isnull()==False)]\n",
    "                stopped_list2.append(stopped)\n",
    "        stopped_df2= pd.DataFrame()\n",
    "        for item in stopped_list2:\n",
    "            stopped_df2 =  stopped_df2.append(item, ignore_index=True)\n",
    "        stopped_df2.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\Stopped\\{}_{}hour_part2.csv'.format(day,hour),index=False)\n",
    "#31-45 minutes dataset\n",
    "        try:\n",
    "            df3=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\{}_{}hour_part3.csv'.format(day,hour))\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        stopped_list3=[]\n",
    "        for i in range(31,46):\n",
    "            if 'lat_%s' %(i+1) in df3.columns:\n",
    "                stopped=df3[(df3['lat_%s' %i].isnull()==True) & (df3['lat_%s' %(i+1)].isnull()==False)]\n",
    "                stopped_list3.append(stopped)\n",
    "        stopped_df3= pd.DataFrame()\n",
    "        for item in stopped_list3:\n",
    "            stopped_df3 =  stopped_df3.append(item, ignore_index=True)\n",
    "        stopped_df3.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\Stopped\\{}_{}hour_part3.csv'.format(day,hour),index=False)\n",
    "#46-00 minutes dataset\n",
    "        try:\n",
    "            df4=pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\{}_{}hour_part4.csv'.format(day,hour))\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        stopped_list4=[]\n",
    "        for i in range(46,60):\n",
    "            if 'lat_%s' %(i+1) in df4.columns:\n",
    "                stopped=df4[(df4['lat_%s' %i].isnull()==True) & (df4['lat_%s' %(i+1)].isnull()==False)]\n",
    "                stopped_list4.append(stopped)\n",
    "        stopped_df4= pd.DataFrame()\n",
    "        for item in stopped_list4:\n",
    "            stopped_df4 =  stopped_df4.append(item, ignore_index=True)\n",
    "        stopped_df4.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\Stopped\\{}_{}hour_part4.csv'.format(day,hour),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c7158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filteredTripStops(day):\n",
    "    for hour in range(0,24): \n",
    "        for fileID in range(1,5):\n",
    "            try:\n",
    "                df= pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\Stopped\\{}_{}hour_part{}.csv'.format(day,hour,fileID))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            Column_num=len(df.columns)\n",
    "            value_df = df.iloc[:, 1:Column_num]\n",
    "            new_list=[]\n",
    "\n",
    "            for i in range((len(df)-1),-1,-1):\n",
    "                row=value_df.loc[i]\n",
    "                new_row=row[row.isnull().shift(3).fillna(False)]\n",
    "                new_list.append(new_row)\n",
    "\n",
    "            new_df= pd.DataFrame()\n",
    "            for item in  new_list:\n",
    "                new_df =  new_df.append(item, ignore_index=True)\n",
    "\n",
    "            first_column= df.iloc[:, 0]\n",
    "            rvs_first_column =first_column[::-1].reset_index(drop=True)\n",
    "            df_all_cols = pd.concat([rvs_first_column,new_df], axis = 1)\n",
    "            df_all_cols.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\Stopped\\Filtered_stops\\{}_{}hour_part{}.csv'.format(day,hour,fileID),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76553f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDateTime(day):\n",
    "    for hour in range(0,24): \n",
    "        for partID in range(1,5):\n",
    "            try:\n",
    "                df= pd.read_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\Stopped\\Filtered_stops\\{}_{}hour_part{}.csv'.format(day,hour,partID))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            col_names=[]\n",
    "            datetime_list=[]\n",
    "            for col in df.columns:\n",
    "                col_names.append(col)\n",
    "            unique_col_names=col_names[1:][::3]\n",
    "            #take only lat_ like names\n",
    "            #uni_name_count=len(unique_col_names)\n",
    "            for names in unique_col_names:\n",
    "                minute=names.split(\"_\")[1]\n",
    "                date_time=str(day)+\":\"+str(hour)+\":\"+str(minute)\n",
    "                datetime_list.append(date_time)\n",
    "            #print(datetime_list)\n",
    "\n",
    "            for item in datetime_list:\n",
    "                col_common_name=item.split(\":\")[2]\n",
    "                item_index=datetime_list.index(item)\n",
    "                location =int(item_index+1)*4\n",
    "                column_name=\"date:time_\"+col_common_name\n",
    "                value=item\n",
    "                #df['date:time_1']=np.where(df['range_1'].notnull(),'value',np.NaN)\n",
    "                #print(location)\n",
    "                try:\n",
    "                    df.insert(location, column_name, value)\n",
    "                except ValueError:\n",
    "                    print(\"value error in \"+ str(day)+\"_\"+str(hour)+\"hour_part\"+str(partID)+\" file. ->>>>\"+str(value))\n",
    "                    continue\n",
    "                except IndexError:\n",
    "                    print(\"index error in \"+ str(day)+\"_\"+str(hour)+\"hour_part\"+str(partID)+\" file. ->>>>\"+str(value))\n",
    "                    continue\n",
    "                date_column_name=\"date:time_\"+col_common_name\n",
    "                range_column_name=\"range_\"+col_common_name\n",
    "                #df['date:time_1']=np.where(df['range_1'].notnull(),'value',np.NaN)\n",
    "                df[date_column_name]=np.where(df[range_column_name].notnull(),value,np.NaN)\n",
    "            df.to_csv(r'C:\\Melbourne_Escooter\\RealDataset\\Results\\July\\Stopped\\Filtered_stops\\WithDateTime\\{}_{}hour_part{}_dateAdded.csv'.format(day,hour,partID),index=False)\n",
    "           # print(\"added!!!!!!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c0af1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
